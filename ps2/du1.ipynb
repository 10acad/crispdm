{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Data Minining Project for a non-profit organisation `UCOUNT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The CRISP-DM Framework\n",
    "\n",
    "The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology. \n",
    "\n",
    "- Business understanding (BU): Determine Business Objectives, Assess Situation, Determine Data Mining Goals, Produce Project Plan\n",
    "\n",
    "- Data understanding (DU): Collect Initial Data, Describe Data, Explore Data, Verify Data Quality\n",
    "\n",
    "- Data preparation (DP): Select Data, Clean Data, Construct Data, Integrate Data\n",
    "\n",
    "- Modeling (M): Select modeling technique, Generate Test Design, Build Model, Assess Model\n",
    "\n",
    "- Evaluation (E): Evaluate Results, Review Process, Determine Next Steps\n",
    "\n",
    "- Deployment (D): Plan Deployment, Plan Monitoring and Maintenance, Produce Final Report, Review Project\n",
    "\n",
    "References: \n",
    "1. [What is the CRISP-DM methodology?](https://www.sv-europe.com/crisp-dm-methodology/)\n",
    "\n",
    "2. [Introduction to CRISP DM Framework for Data Science and Machine Learning](https://www.linkedin.com/pulse/chapter-1-introduction-crisp-dm-framework-data-science-anshul-roy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Role\n",
    "All of the tasks you are asked to do in this course assumes that you are leading a data science group for a non-profit (fake) company called UCOUNT (yoUr CONTribution). \n",
    "\n",
    "As a non-profit organisation, UCOUNT's main income is donation. Moreover, for some of the smaller projects, UCOUNT uses the Kickstart and other crowd funding platforms to raise funding. \n",
    "\n",
    "As data science group leader at UCOUNT, you are tasked to advise the marketing department such that they run an effective donation and kickstart campaigns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets\n",
    "You have two sets of data at your disposal. \n",
    "\n",
    "1. The first one is a collection of crowd funding campaigns performed in the past by different people and organisations at the [Kickstarter website](https://www.kickstarter.com/). The data is obtained using web scraper robots run by [webrobots](https://webrobots.io/kickstarter-datasets/). The bots crawled all Kickstarter projects and collect data which are then dumped in CSV format.\n",
    "\n",
    "   You can download Kickstarter data set here: [kickstarter-cleaned.csv](https://github.com/flavianh/kickstarter-dash/blob/master/kickstarter-cleaned.csv)\n",
    "   \n",
    "   You will use this data to understand the important features of successful crowd funding campaigns. For example what are the common characteristics of successful campaigns?\n",
    "\n",
    "2. The second data is a Census Income Data Set, which comes from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Census+Income). The datset was donated by Ron Kohavi and Barry Becker, after being published in the article _\"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\"_. You can find the article by Ron Kohavi [online](https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf).\n",
    "\n",
    "   You can download Census Income Data Set data here: [census-income-1996.csv](s3://10ac-courses-data/csv/census-income-1996.csv)\n",
    "    \n",
    "   You will use this data set to predict the annual income of a target donor. Understanding an individual's income can help a non-profit company like UCOUNT better understand how large of a donation to request, or whether or not they should reach out to begin with.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "----\n",
    "In this part of the CRISP-DM phase, you will explore the data to understand the feature space, the data attributes present at training and testing time, and the target label, which is used to train and validate a model.  \n",
    "\n",
    "A good code is reusable. That means you write functions and use them repeatedly for different data analysis projects. One good example the **sklearn** python package. As you may know, most of the algorithms, and other necessary processes necessary for most machine learning activities are written in this package such that millions other users don't have to repeat them.\n",
    "\n",
    "It is also highly recommended that you write python functions to do most of your customised plots, data loading, data cleaning and other activities you do repeatedly . That way you will save a lot of time in the future, and your code becomes readable. \n",
    "\n",
    ">**Note:** you can save a bunch of fuctions defined in a cell to a file for later use using the jupyter notebook magic\n",
    "\n",
    "    %%file /path/filename.py \n",
    "\n",
    ">You can then load a specific fuction from the pythin file using \n",
    "\n",
    "    sys.path.append(/path)\n",
    "    from filename import function  \n",
    ">To learn more about the different notebook magic functions and other jupyter notebook tricks, see  \n",
    "<a href=https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/> 28 Jupyter Notebook Tips, Tricks, and Shortcuts</a>  or <a href=https://towardsdatascience.com/jupyter-notebook-hints-1f26b08429ad>Boosting Your Jupyter Notebook Productivity</a>\n",
    "\n",
    "To motivate you in that directions, a few functions are defined below that are useful to download data from reomote url, and check for existing data, create folder if they are not in the local file system etc. There are also a couple of functions for ploting histogram distributions from a DataFrame and print results. An example of how to save notebook cells to a file is also given below.\n",
    "\n",
    "Since the notes in these lecture series are brief, we strongly recommend you refer to online lectures, blogs, and data science text books. Some good references are listed below. Moreover, please if you don't understand something be proactive and ask questions in 10 Academy and/or stackoverflow forums. \n",
    "\n",
    "You should make sure that with every exercise, you are building a strong mix of data science knowledgg, which encompases mathematical and statistical concepts and algorithms,  and data science hard skill, writing good code and getting familiar with important data science packages such as scipy, pandas, sklearn, Keras, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#this part means allows jupyter to automatically load modules modified after being load\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "mypyrootdir = os.path.join(home, 'mypy')\n",
    "\n",
    "#add mypydir in system path. This will allow us to import files easily\n",
    "if not mypyrootdir in sys.path:\n",
    "    sys.path.insert(0, mypyrootdir)\n",
    "\n",
    "#folder to save python files for later use as modules\n",
    "def create_subfolder_in_mypydir(path,\n",
    "                                rootdir=os.path.expanduser(\"~\"), \n",
    "                                ismodule=True):\n",
    "    '''\n",
    "    Create folder to save python files relative to home mypydir(~/mypy/tenx)\n",
    "    if submodule is True, then the folder will be turned to python module\n",
    "    by adding an empty ~/mypy/tenx/path/__init__.py file\n",
    "    '''\n",
    "    dirpath = os.path.join(rootdir,path)\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "    if ismodule:\n",
    "        with open(os.path.join(dirpath, '__init__.py'), 'w') as f:\n",
    "            f.write('')\n",
    "        \n",
    "    return dirpath\n",
    "\n",
    "\n",
    "\n",
    "#Create the tenac mypydir directory to save python functions for use in many projects\n",
    "tenxdir = create_subfolder_in_mypydir('tenx', rootdir=mypyrootdir)\n",
    "                                      \n",
    "#create util directory inside tenxdir\n",
    "tenxutil = create_subfolder_in_mypydir('util', rootdir=tenxdir)\n",
    "\n",
    "#import the python modules in the tenx folder\n",
    "import tenx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#this is to save the next cell, which defines some python functions, to a file\n",
    "ofilename = os.path.join(tenxutil,'mycurl.py')\n",
    "ofilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%file $ofilename\n",
    "\n",
    "import requests\n",
    "import os, sys\n",
    "                    \n",
    "def get_filename_from_cd(cd):\n",
    "    \"\"\"\n",
    "    Get filename from content-disposition\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0]\n",
    "\n",
    "def is_fsize_larger(header, size=200, unit=1e6): \n",
    "    # size is in units of 1e6 - MegaByte (Mb)\n",
    "    \n",
    "    content_length = header.get('content-length', None)\n",
    "    \n",
    "    if content_length and content_length > size*unit:  # 200 mb approx\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def is_downloadable(url, size=None, unit=1e6):\n",
    "    \"\"\"\n",
    "    Does the url contain a downloadable resource?\n",
    "    To answer this question, we first fetch the headers \n",
    "    of a url before actually downloading it.\n",
    "    This allows us to skip downloading files which \n",
    "    weren't meant to be downloaded    \n",
    "    \"\"\"    \n",
    "    h = requests.head(url, allow_redirects=True)\n",
    "    header = h.headers\n",
    "    \n",
    "    unitd = {1e3:'Kb', 1e6:'Mb', 1e9:'Gb', 1e12:'Tb', 1e16:'Pb'}[unit]\n",
    "    \n",
    "    dload = True \n",
    "    if size: #if size is given (not None)        \n",
    "        dload = is_fsize_larger(header, size=size, unit=unit)\n",
    "        \n",
    "    if not dload: #file size condition\n",
    "        print('file size of %s is more than %s %s'% (url, size, unitd) )\n",
    "        return False\n",
    "    \n",
    "    #print('url=%s HEADER'%url)\n",
    "    #print(header)\n",
    "    \n",
    "    content_type = header.get('content-type')    \n",
    "    #if 'text' in content_type.lower(): #text condition\n",
    "    #    print('%s is a link to plain text - not data'%url)\n",
    "    #    return False\n",
    "    if 'html' in content_type.lower(): #html condition\n",
    "        print('%s is a link to plain html - not data'%url)\n",
    "        return False\n",
    "              \n",
    "    return True\n",
    "\n",
    "def make_dir_ifnew(filename, isdir=False, rfile=True, rtest=True):\n",
    "    if os.path.exists(filename):\n",
    "        if rfile and rtest:\n",
    "            return filename, True\n",
    "        elif rfile and not rtest:\n",
    "            return filename\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    if isdir: #if filename is directory path\n",
    "        dirname = filename\n",
    "    else: #filename is path+filename.ext\n",
    "        dirname = os.path.dirname(filename)\n",
    "        \n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "        print('created directory: %s'%dirname)\n",
    "    else:\n",
    "        print('directory %s already exists.'%dirname)\n",
    "        \n",
    "    if rfile and rtest:\n",
    "        return filename, True\n",
    "    elif rfile and not rtest:\n",
    "        return filename    \n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "def download_data(url, filename, replace=False):\n",
    "        \n",
    "    #make directory if it does not exist\n",
    "    file_exists = make_dir_ifnew(filename,rfile=False)\n",
    "    \n",
    "    # only download if file doesn't exist or replace=True\n",
    "    if replace or not file_exists: \n",
    "        dload = is_downloadable(url)              \n",
    "        print('Is the url=%s downloadable: %s' % (url,dload) )\n",
    "\n",
    "        if dload:\n",
    "            #request HTTP GET\n",
    "            r = requests.get(url)\n",
    "\n",
    "            #write the data obtained to file\n",
    "            open(filename, 'wb').write(r.content)\n",
    "\n",
    "            print('url successfully downloaded to file: %s'%filename)\n",
    "        else:\n",
    "            print('url can not be downloaded')\n",
    "    else:\n",
    "        print('%s already exists - downloading not necessary'%filename)\n",
    "              \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def feature_plot(importances, X_train, y_train):\n",
    "    \n",
    "    # Display the five most important features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    columns = X_train.columns.values[indices[:5]]\n",
    "    values = importances[indices][:5]\n",
    "\n",
    "    # Creat the plot\n",
    "    fig = plt.figure(figsize = (9,5))\n",
    "    pl.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\n",
    "    plt.bar(np.arange(5), values, width = 0.6, align=\"center\", color = '#00A000', \\\n",
    "          label = \"Feature Weight\")\n",
    "    plt.bar(np.arange(5) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n",
    "          label = \"Cumulative Feature Weight\")\n",
    "    plt.xticks(np.arange(5), columns)\n",
    "    plt.xlim((-0.5, 4.5))\n",
    "    plt.ylabel(\"Weight\", fontsize = 12)\n",
    "    plt.xlabel(\"Feature\", fontsize = 12)\n",
    "    \n",
    "    plt.legend(loc = 'upper center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate(results, accuracy, f1):\n",
    "    \"\"\"\n",
    "    Visualization code to display results of various learners.\n",
    "    \n",
    "    inputs:\n",
    "      - learners: a list of supervised learners\n",
    "      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n",
    "      - accuracy: The score for the naive predictor\n",
    "      - f1: The score for the naive predictor\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(2, 3, figsize = (11,7))\n",
    "\n",
    "    # Constants\n",
    "    bar_width = 0.3\n",
    "    colors = ['#A00000','#00A0A0','#00A000']\n",
    "    \n",
    "    # Super loop to plot four panels of data\n",
    "    for k, learner in enumerate(results.keys()):\n",
    "        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n",
    "            for i in np.arange(3):\n",
    "                \n",
    "                # Creative plot code\n",
    "                ax[j//3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n",
    "                ax[j//3, j%3].set_xticks([0.45, 1.45, 2.45])\n",
    "                ax[j//3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n",
    "                ax[j//3, j%3].set_xlabel(\"Training Set Size\")\n",
    "                ax[j//3, j%3].set_xlim((-0.1, 3.0))\n",
    "    \n",
    "    # Add unique y-labels\n",
    "    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[0, 1].set_ylabel(\"Accuracy Score\")\n",
    "    ax[0, 2].set_ylabel(\"F-score\")\n",
    "    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[1, 1].set_ylabel(\"Accuracy Score\")\n",
    "    ax[1, 2].set_ylabel(\"F-score\")\n",
    "    \n",
    "    # Add titles\n",
    "    ax[0, 0].set_title(\"Model Training\")\n",
    "    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n",
    "    ax[0, 2].set_title(\"F-score on Training Subset\")\n",
    "    ax[1, 0].set_title(\"Model Predicting\")\n",
    "    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n",
    "    ax[1, 2].set_title(\"F-score on Testing Set\")\n",
    "    \n",
    "    # Add horizontal lines for naive predictors\n",
    "    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    \n",
    "    # Set y-limits for score panels\n",
    "    ax[0, 1].set_ylim((0, 1))\n",
    "    ax[0, 2].set_ylim((0, 1))\n",
    "    ax[1, 1].set_ylim((0, 1))\n",
    "    ax[1, 2].set_ylim((0, 1))\n",
    "\n",
    "    # Create patches for the legend\n",
    "    patches = []\n",
    "    for i, learner in enumerate(results.keys()):\n",
    "        patches.append(mpatches.Patch(color = colors[i], label = learner))\n",
    "    plt.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n",
    "               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n",
    "    \n",
    "    # Aesthetics\n",
    "    plt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def distribution(data, col_names, \n",
    "                 transformed = False,\n",
    "                 figsize = None,\n",
    "                 xra=None,yra=None, \n",
    "                 yticks=None, \n",
    "                 yticklabels=None\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Visualization code for displaying skewed distributions of features\n",
    "    \"\"\"\n",
    "    #\n",
    "    plotInfo = {'nrow':0, 'ncol':0,\n",
    "                'isfigsize':False,                \n",
    "                'isxra':False, 'isyra':False,\n",
    "                'isyticks':False, \n",
    "                'isyticklabels':False,\n",
    "                'istransformed':False}  \n",
    "    \n",
    "    slevel = 0\n",
    "    \n",
    "    # Create figure\n",
    "    if figsize is None:\n",
    "        figsize = (11,5)\n",
    "    else:\n",
    "        plotInfo['isfigsize'] = True\n",
    "        slevel += 2 \n",
    "        \n",
    "    fig = plt.figure(figsize = figsize);\n",
    "    \n",
    "    #depending on col_names size, adject figure rows and columns\n",
    "    nrow = 1 if len(col_names) <= 3 else int( np.ceil(len(col_names)/3) ) \n",
    "    ncol = len(col_names) if len(col_names) <= 3 else 3\n",
    "    plotInfo['nrow'] = nrow\n",
    "    plotInfo['ncol'] = ncol\n",
    "    slevel += nrow*ncol #more plots means advanced use\n",
    "    \n",
    "    # Skewed feature plotting\n",
    "    for i, feature in enumerate(col_names):\n",
    "        ax = fig.add_subplot(nrow, ncol, i+1)\n",
    "        ax.hist(data[feature], bins = 25, color = '#00A0A0')\n",
    "        ax.set_title(\"'%s' Feature Distribution\"%(feature), fontsize = 14)\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Number of Records\")\n",
    "        \n",
    "        if not xra is None: \n",
    "            if type(xra[0]) is not list: \n",
    "                xra = [xra for _ in col_names]\n",
    "            ax.set_xlim(xra[i])\n",
    "            plotInfo['isxra'] = True \n",
    "            slevel += 2    \n",
    "            \n",
    "        if not yra is None: \n",
    "            if type(yra[0]) is not list: \n",
    "                yra = [yra for _ in col_names]\n",
    "            ax.set_ylim(yra[i])\n",
    "            plotInfo['isyra'] = True \n",
    "            slevel += 2\n",
    "            \n",
    "        if not yticks is None: \n",
    "            if type(yticks[0]) is not list: \n",
    "                yticks = [yticks for _ in col_names]\n",
    "            ax.set_yticks(yticks[i])\n",
    "            plotInfo['isyticks'] = True \n",
    "            slevel += 2\n",
    "            \n",
    "        if not yticklabels is None: \n",
    "            if type(yticklabels[0]) is not list: \n",
    "                yticklabels = [yticklabels for _ in col_names]\n",
    "            ax.set_yticklabels(yticklabels[i])\n",
    "            plotInfo['isyticklabels'] = True \n",
    "            slevel += 2\n",
    "\n",
    "    # Plot aesthetics\n",
    "    if transformed:\n",
    "        plotInfo['istransformed'] = True\n",
    "        fig.suptitle(\"Log-transformed Distributions of Continuous Census Data Features\", \\\n",
    "            fontsize = 16, y = 1.03)\n",
    "    else:\n",
    "        fig.suptitle(\"Skewed Distributions of Continuous Census Data Features\", \\\n",
    "            fontsize = 16, y = 1.03)\n",
    "        \n",
    "    fig.tight_layout()\n",
    "\n",
    "    plotInfo['slevel'] = slevel\n",
    "    return plotInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Loading the Data and Understanding the Structure\n",
    "A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. In the code cell below, you will need to compute the following:\n",
    "- The total number of records, `'n_records'`\n",
    "- The number of individuals making more than \\$50,000 annually, `'n_greater_50k'`.\n",
    "- The number of individuals making at most \\$50,000 annually, `'n_at_most_50k'`.\n",
    "- The percentage of individuals making more than \\$50,000 annually, `'greater_percent'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ks_filename='../data/kickstarter-cleaned.csv'\n",
    "census_filename='../data/census-income-1996.csv' \n",
    "\n",
    "#download data if not done already\n",
    "f=download_data('https://raw.githubusercontent.com/flavianh/kickstarter-dash/master/kickstarter-cleaned.csv', \n",
    "              ks_filename)\n",
    "f=download_data('https://raw.githubusercontent.com/ramsaran-vuppuluri/finding_donors/master/census.csv', \n",
    "              census_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d20ca17f332b9d7f87a545482570faa2",
     "grade": false,
     "grade_id": "cell-acbb6f42cd7169e0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO: read census_filename data into pandas data frame as\n",
    "#csdata = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(csdata.head(n = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "342cea9d8295b26a8fd1480e3a524c60",
     "grade": true,
     "grade_id": "cell-db9a12f548ce9710",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Show an example of a record with scaling applied\n",
    "display(csdata.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "** HINT: ** You may need to add a cell above, and look at the DataFrame to understand how the `'income'` entries are formatted. \n",
    "\n",
    "** Featureset Exploration **\n",
    "\n",
    "* **age**: continuous. \n",
    "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. \n",
    "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. \n",
    "* **education-num**: continuous. \n",
    "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n",
    "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. \n",
    "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. \n",
    "* **race**: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. \n",
    "* **sex**: Female, Male. \n",
    "* **capital-gain**: continuous. \n",
    "* **capital-loss**: continuous. \n",
    "* **hours-per-week**: continuous. \n",
    "* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aeb472e8951e4fec650a7eb25c05ca52",
     "grade": false,
     "grade_id": "cell-6fd9b4d551a8b764",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: get total number of records (rows) and number of features (columns) as\n",
    "#n_records = \n",
    "#n_features = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Number of records where individual's income is more than $50,000\n",
    "n_greater_50k = len(csdata[csdata.income =='>50K'])\n",
    "\n",
    "# TODO: Number of records where individual's income is at most $50,000\n",
    "#n_at_most_50k = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# TODO: Percentage of individuals whose income is more than $50,000\n",
    "#greater_percent = \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of records: {}\".format(n_records))\n",
    "print(\"Total number of features: {}\".format(n_features))\n",
    "print(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\n",
    "print(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\n",
    "print(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a61b0a67efb055046992162f7866c3d0",
     "grade": true,
     "grade_id": "cell-a0b289a36460b035",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "assert n_records==45222\n",
    "assert n_features==14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Understanding the Data Types of the different Features (Dimensions)\n",
    "The first task of any Machine Learning project is to understand the data types, distributions and other important describtive stats of features. Before proceeding futher explore the feature space and make sure you are familiar with the nature of the data. To help you do so, by the end of you exploration you should define the following variables that will be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fe83ca4868f4eb667570ac331801ca48",
     "grade": false,
     "grade_id": "cell-227fe84c35fb3418",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: define variables which contains list of columns that are numerical and categorial\n",
    "#numerical_col_names = \n",
    "#categorial_col_names = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aed8afcc7b859145e608335297261c5",
     "grade": true,
     "grade_id": "cell-1f6ad44d29eb58b7",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "assert numerical_col_names == ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "----\n",
    "## Preparing the Data for ML\n",
    "Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as **preprocessing**. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Split Data to Features and Target Label \n",
    "Before proceeding further, let us be clear that the main task of the income data project is to predict if a given an individual described by a given cobmination of features has an yearly income of USD '>=50K' or '<50K'. These values are given in the `income` column of the training data.\n",
    "\n",
    "This means the 'income' column is the target label while all the other columns can be considered as features that describe a sample (you can think of them as coordinates, hence the number of independent features are the numbner of dimensions).\n",
    "\n",
    "What are the number of dimensions of the income data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "afee12860e158a4e6b4f64d1fd012b09",
     "grade": false,
     "grade_id": "cell-af3eb32b8cbd6faf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Split the data into features and target label. Use the income_raw \n",
    "#income_raw =  #for target labels\n",
    "#features_raw =  #for features\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5adeca49e65840cf2abdc790dcf9891",
     "grade": true,
     "grade_id": "cell-ebac3e4ce81022aa",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(income_raw.shape, features_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Transforming Skewed Continuous Features\n",
    "A dataset may sometimes contain features whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Your next investigation is to explore the feature space to find out which of the features are highly skewed. In particular, list the top two highly skewd numerical features in the census dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1282531adb28ec071542babb2b42dfcf",
     "grade": false,
     "grade_id": "cell-5bdae80c869aef53",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#list the name of two numerical features that are most skewed by distribution\n",
    "#top2_skewed = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# What are the approperiate data ranges (min, max) to use  \n",
    "# the top2_skwed_col_names variable above?   \n",
    "# top2_skew_ranges = [[],[]]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Testing your python skill\n",
    "# TODO: what are the non-optional and optional \n",
    "# parameters in the function 'distribution()' defined above?\n",
    "\n",
    "\n",
    "# using the distribution function given above, plot histogram distributions\n",
    "# of the top two skewed continuous features. You should pass at least one \n",
    "# optional features to the funation to complete this task. \n",
    "# The output of distribution() should be assigned to plotInfo variable like\n",
    "# pinfo = distribution()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc2700cdb838572e225a275c868a9097",
     "grade": true,
     "grade_id": "cell-3f4c3033f215ab61",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Logarithmic Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "For highly-skewed feature distributions, it is common practice to apply a <a href=\"https://en.wikipedia.org/wiki/Data_transformation_(statistics)\">logarithmic transformation</a> on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. \n",
    "\n",
    ">**Deep Thinking:** \n",
    "\n",
    ">What does a logarithmic transformation do to a distribution? How does the distance between two points change once the data is log transformed?\n",
    "\n",
    "> As you may guess log transformation is not always a idea even if a feature is skewed. Can you think of examples where log tranformation affects badly machine learning? Hint, if two features are correlated, and one of the feature is highly skewed while the other is not, what happens if you apply log transformation on the skewed feature?\n",
    "\n",
    "Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of `0` or negative values are undefined, so we must translate the values by a small amount above `0` to apply the the logarithm successfully.\n",
    "\n",
    "Run the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Log-transform the skewed features\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[top2_skewed] = features_raw[top2_skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# Visualize the new log distributions\n",
    "tpinfo = distribution(features_log_transformed,top2_skewed, transformed = True,\n",
    "                      xra = [0.1,0.3], yra = [0,4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb4d8dfbeda5fd9f406e58e1099acabe",
     "grade": true,
     "grade_id": "cell-b1e13d5f7f839d58",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Normalizing Numerical Features\n",
    "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution; however, normalization ensures that each feature is treated equally when applying supervised learners. \n",
    "\n",
    ">**Deep Thinking:** \n",
    "\n",
    ">What does normalization do to the \"units\" of features? For example what is the unit of age once it is normalized to be in the range (0, 1)?\n",
    "\n",
    ">In which scenario do you think normalization helps? and when is it not a good strategy? \n",
    "\n",
    "Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.\n",
    "\n",
    "Run the code cell below to normalize each numerical feature. We will use [`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[numerical_col_names] = scaler.fit_transform(\n",
    "    features_log_transformed[numerical_col_names])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### One-hot encoding of categorical variables\n",
    "\n",
    "From the **tables** above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called *categorical variables*) be converted. One popular way to convert categorical variables is by using the **one-hot encoding** scheme. One-hot encoding creates a _\"dummy\"_ variable for each possible category of each non-numeric feature. \n",
    "\n",
    "For example, assume a feature with a header `Name` has three possible entries: `A`, `B`, or `C`. We then encode this feature into `Name_A`, `Name_B` and `Name_C`.\n",
    "\n",
    "     | Name  |             ----> one-hot encoding ---->   | Name_A | Name_B | Name_C | \n",
    "     |  B    |             ----> one-hot encoding ---->   | 0      | 1      | 0      |  \n",
    "     |  C    |             ----> one-hot encoding ---->   | 0      | 0      | 1      |  \n",
    "     |  A    |             ----> one-hot encoding ---->   | 1      | 0      | 0      |  \n",
    "\n",
    "Additionally, as with the non-numeric features, we need to convert the non-numeric target label, `'income'` to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as `0` and `1`, respectively. \n",
    "\n",
    "In code cell below, we will do the following:\n",
    " - Use [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) to perform one-hot encoding on the `'features_log_minmax_transform'` data.\n",
    " - Convert the target label `'income_raw'` to numerical entries.\n",
    "   - Set records with \"<=50K\" to `0` and records with \">50K\" to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "\n",
    "income = pd.Series(data=income_raw)\n",
    "income = income.replace('<=50K',0)\n",
    "income = income.replace('>50K',1)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(features_final.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "\n",
    "# Uncomment the following line to see the encoded feature names\n",
    "# print encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Shuffle and Split Data\n",
    "Now all _categorical variables_ have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.\n",
    "\n",
    "Run the code cell below to perform this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    income, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    " \n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Save the Train and Test sets for future analysis\n",
    "The the train and test sets into files so that you can read them from another notebook.\n",
    "\n",
    "In the cell code below, you are required to implement the missing code in the save_train_test_split() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c03f5dcabdde090c2613b48a81885d1f",
     "grade": false,
     "grade_id": "cell-66a4fab0cff88b98",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_split_samples(train=True, test=True, validate=False,\n",
    "                        path='./'):\n",
    "    '''\n",
    "        This function loads the training, testing and/or validating\n",
    "        samples if the parameters train, test and validate are True.\n",
    "        The order of the output is as follows:\n",
    "        \n",
    "        if train=True and test=False, validate=False \n",
    "        samples=[X_train, y_train], \n",
    "        \n",
    "        if train=True and test=True, validate=False \n",
    "        samples=[X_train, y_train, X_test, y_test], \n",
    "        \n",
    "        if train=True and test=True, validate=True\n",
    "        samples=[X_train, y_train, X_test, y_test, X_validate, y_validate]\n",
    "        \n",
    "        All samples are read as pandas DataFrame.        \n",
    "    '''\n",
    "    join = os.path.join\n",
    "    samples = []\n",
    "    if train:\n",
    "        fname_X_train,fexist1 = make_dir_ifnew(join(path,'train/features.csv'))        \n",
    "        fname_y_train,fexist2 = make_dir_ifnew(join(path,'train/label.csv'))\n",
    "        if fexist1 and fexist2:\n",
    "            samples.append(pd.read_csv(fname_X_train,index_col=0))\n",
    "            samples.append(pd.read_csv(fname_y_train,index_col=0))\n",
    "        else:\n",
    "            print('Error: Testing sample can not be load!')            \n",
    "\n",
    "    if test:\n",
    "        fname_X_test,fexist1 = make_dir_ifnew(join(path,'test/features.csv'))\n",
    "        fname_y_test,fexist2 = make_dir_ifnew(join(path,'test/label.csv'))\n",
    "        if fexist1 and fexist2:\n",
    "            samples.append(pd.read_csv(fname_X_test,index_col=0))\n",
    "            samples.append(pd.read_csv(fname_y_test,index_col=0))    \n",
    "        else:\n",
    "            print('Error: Testing sample can not be load!')            \n",
    "\n",
    "    if validate:\n",
    "        fname_X_validate,fexist1 = make_dir_ifnew(join(path,'validate/features.csv'))\n",
    "        fname_y_validate,fexist2 = make_dir_ifnew(join(path,'validate/label.csv'))\n",
    "        if fexist1 and fexist2:\n",
    "            samples.append(pd.read_csv(fname_X_validate,index_col=0))\n",
    "            samples.append(pd.read_csv(fname_y_validate,index_col=0))\n",
    "        else:\n",
    "            print('Error: Validation sample can not be load!')\n",
    "                  \n",
    "    return samples\n",
    "    \n",
    "    \n",
    "def save_train_test_split(train_sample, test_sample, \n",
    "                        validate_sample=None,\n",
    "                          path='./'):\n",
    "    '''\n",
    "        This function saves training features and labels passed as\n",
    "        train_samples=[X_train, y_train], \n",
    "        in the path/train folder in separate CSV files.\n",
    "        \n",
    "        Similarly if the test_samples = [X_test, y_test] are given,  \n",
    "        it saves it in the path/test folder in CSV format.\n",
    "        \n",
    "        If the optional validate=[X_validate, y_validate] is passed, \n",
    "        it saves it in the path/validate folder in CSV format.\n",
    "    '''\n",
    "    join = os.path.join\n",
    "    fname_X_train = make_dir_ifnew(join(path,'train/features.csv'), rtest=False)\n",
    "    fname_y_train = make_dir_ifnew(join(path,'train/labels.csv'), rtest=False)\n",
    "\n",
    "    fname_X_test = make_dir_ifnew(join(path,'test/features.csv'), rtest=False)\n",
    "    fname_y_test = make_dir_ifnew(join(path,'test/labels.csv'), rtest=False)\n",
    "\n",
    "    if not validate_sample is None:\n",
    "        fname_X_validate = make_dir_ifnew(join(path,'validate/features.csv'), rtest=False)\n",
    "        fname_y_validate = make_dir_ifnew(join(path,'validate/labels.csv'), rtest=False)    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return 'train test split successfully saved to %s'%path\n",
    "\n",
    "\n",
    "savepath = './'\n",
    "\n",
    "#use save_train_test_split function to tsave the trainning and testing samples in the current directory\n",
    "status = save_train_test_split([X_train, y_train], \n",
    "                      test_sample = [X_test, y_test], \n",
    "                      path=savepath)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68759ed8180ea758266c2499ad334ed5",
     "grade": true,
     "grade_id": "cell-b04b58f4de1fef03",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#let us check if the dataframes you saved to a file and read are the same as the original\n",
    "\n",
    "#read saved files\n",
    "X_train2, y_train2,X_test2, y_test2 =  load_split_samples(train=True, test=True, path=savepath)\n",
    "\n",
    "\n",
    "assert all(X_train==X_train2)\n",
    "assert all(X_test==X_test2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
